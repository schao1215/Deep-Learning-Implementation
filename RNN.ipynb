{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load labeled data from a file\n",
    "def load_labeled_data(filename):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('+++$+++')\n",
    "            sentiment = int(parts[0].strip())\n",
    "            sentence = parts[-1].strip()\n",
    "            labels.append(sentiment)\n",
    "            sentences.append(sentence)\n",
    "    return sentences, labels\n",
    "\n",
    "# Load the labeled data\n",
    "labeled_sentences, labeled_labels = load_labeled_data('./data/training_label.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load unlabeled data from a file\n",
    "def load_unlabeled_data(filename):\n",
    "    sentences = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentence = line.strip()\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# Load the unlabeled data\n",
    "unlabeled_sentences = load_unlabeled_data('./data/training_nolabel.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load testing data from a file\n",
    "def load_testing_data(filename):\n",
    "    indices = []\n",
    "    sentences = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        next(file)  # Skip the header line\n",
    "        for line in file:\n",
    "            parts = line.strip().split(',')\n",
    "            index = int(parts[0].strip())\n",
    "            sentence = ','.join(parts[1:]).strip()\n",
    "            indices.append(index)\n",
    "            sentences.append(sentence)\n",
    "    return indices, sentences\n",
    "\n",
    "# Load the testing data\n",
    "test_indices, test_sentences = load_testing_data('./data/testing_data.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/seanchao/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding vector for 'good': [-4.2089453   1.0795008   1.612712    0.8321846   0.70990443 -2.886136\n",
      " -0.02445051  2.285883    2.040423   -1.1351883   0.69239914  0.37003493\n",
      " -0.06859332 -0.7411943  -3.281865   -0.92523444 -1.0577872   3.036679\n",
      "  1.4631374   1.2296252   2.165102    0.95393723  1.7943482   0.4486362\n",
      "  0.88647056 -0.22379524  3.743727    0.1147678  -2.0777636   0.09334759\n",
      "  1.9756734   2.8486109   0.39717868 -1.0790299  -0.61531323 -0.06802832\n",
      "  1.8414276   0.03989493  1.494306   -3.0175202   0.12096179 -3.1852353\n",
      "  0.3780883   1.8745061  -3.423817   -3.3172634   0.05231145  1.1028816\n",
      "  2.1748257   0.27270886  2.158465    0.5370942   0.277179    2.7597613\n",
      " -0.8803218   3.724905   -0.14433813 -0.5172164  -3.1422622  -1.7458636\n",
      " -2.9340186  -2.1259954  -1.3340582  -2.9028647  -2.9530919   0.28637993\n",
      "  2.6422462   0.5689596  -1.8673733  -1.2289314  -0.8435594   0.9949758\n",
      " -0.26813826  1.8095034  -1.140273    1.9218198  -2.0627234  -0.66035116\n",
      " -1.3337122  -0.45198643 -0.66185     1.8742603   3.0578544   2.6583316\n",
      " -2.6886334  -0.06105777  1.0300841  -2.0512033   2.3934968  -0.73833376\n",
      " -1.7794673  -2.6497436   1.6325734   0.14233172 -2.464741    2.3671536\n",
      "  3.1971834  -1.5946611  -0.08663166  2.8492026 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the sentences\n",
    "labeled_tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in labeled_sentences]\n",
    "unlabeled_tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in unlabeled_sentences]\n",
    "test_tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in test_sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "all_tokenized_sentences = labeled_tokenized_sentences + unlabeled_tokenized_sentences + test_tokenized_sentences\n",
    "word2vec_model = Word2Vec(sentences=all_tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Map words to word embeddings\n",
    "word_embeddings = {}\n",
    "for word in word2vec_model.wv.index_to_key:\n",
    "    word_embeddings[word] = word2vec_model.wv[word]\n",
    "\n",
    "# Example: Print the word embedding vector for a specific word\n",
    "print(\"Word embedding vector for 'good':\", word_embeddings['good'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
