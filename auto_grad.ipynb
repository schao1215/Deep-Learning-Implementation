{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# torch里要求数据类型必须是float\n",
    "x = np.arange(1, 12, dtype=np.float32).reshape(-1, 1)\n",
    "y = 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 继承nn.module，实现前向传播，线性回归直接可以看做是全连接层\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()  # 继承父类方法\n",
    "        self.linear = nn.Linear(input_dim, output_dim)  # 定义全连接层，其中input_dim和output_dim是输入和输出数据的维数\n",
    "\n",
    "    # 定义前向传播算法\n",
    "    def forward(self, inp):\n",
    "        out = self.linear(inp)  # 输入x后，通过全连接层得到输入出结果out\n",
    "        return out  # 返回被全连接层处理后的结果\n",
    "\n",
    "\n",
    "# 定义线性回归模型\n",
    "regression_model = LinearRegressionModel(1, 1)  # x和y都是一维的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以通过to()或者cuda()使用GPU进行模型的训练，需要将模型和数据都转换到GPU上，也可以指定具体的GPU，如.cuda(1)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "regression_model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000  # 训练次数\n",
    "learning_rate = 0.01  # 学习速率\n",
    "optimizer = torch.optim.SGD(regression_model.parameters(), learning_rate)  # 优化器（未来会详细介绍），这里使用随机梯度下降算法（SGD）\n",
    "criterion = nn.MSELoss()  # 使用均方误差定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGD in module torch.optim.sgd:\n",
      "\n",
      "class SGD(torch.optim.optimizer.Optimizer)\n",
      " |  SGD(params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize: bool = False, foreach: Optional[bool] = None, differentiable: bool = False)\n",
      " |  \n",
      " |  Implements stochastic gradient descent (optionally with momentum).\n",
      " |  \n",
      " |  .. math::\n",
      " |     \\begin{aligned}\n",
      " |          &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      " |          &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
      " |              \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
      " |          &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n",
      " |          \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      " |          &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
      " |          &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
      " |          &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n",
      " |          &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
      " |          &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n",
      " |          &\\hspace{10mm}\\textbf{else}                                                          \\\\\n",
      " |          &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n",
      " |          &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n",
      " |          &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                             \\\\\n",
      " |          &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n",
      " |          &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n",
      " |          &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n",
      " |          &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n",
      " |          &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      " |          &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      " |     \\end{aligned}\n",
      " |  \n",
      " |  Nesterov momentum is based on the formula from\n",
      " |  `On the importance of initialization and momentum in deep learning`__.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
      " |          parameter groups\n",
      " |      lr (float, optional): learning rate (default: 1e-3)\n",
      " |      momentum (float, optional): momentum factor (default: 0)\n",
      " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      " |      dampening (float, optional): dampening for momentum (default: 0)\n",
      " |      nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
      " |      maximize (bool, optional): maximize the objective with respect to the\n",
      " |          params, instead of minimizing (default: False)\n",
      " |      foreach (bool, optional): whether foreach implementation of optimizer\n",
      " |          is used. If unspecified by the user (so foreach is None), we will try to use\n",
      " |          foreach over the for-loop implementation on CUDA, since it is usually\n",
      " |          significantly more performant. Note that the foreach implementation uses\n",
      " |          ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n",
      " |          being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n",
      " |          parameters through the optimizer at a time or switch this flag to False (default: None)\n",
      " |      differentiable (bool, optional): whether autograd should\n",
      " |          occur through the optimizer step in training. Otherwise, the step()\n",
      " |          function runs in a torch.no_grad() context. Setting to True can impair\n",
      " |          performance, so leave it False if you don't intend to run autograd\n",
      " |          through this instance (default: False)\n",
      " |  \n",
      " |  \n",
      " |  Example:\n",
      " |      >>> # xdoctest: +SKIP\n",
      " |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      " |      >>> optimizer.zero_grad()\n",
      " |      >>> loss_fn(model(input), target).backward()\n",
      " |      >>> optimizer.step()\n",
      " |  \n",
      " |  __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      " |  \n",
      " |  .. note::\n",
      " |      The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      " |      Sutskever et. al. and implementations in some other frameworks.\n",
      " |  \n",
      " |      Considering the specific case of Momentum, the update can be written as\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n",
      " |      parameters, gradient, velocity, and momentum respectively.\n",
      " |  \n",
      " |      This is in contrast to Sutskever et. al. and\n",
      " |      other frameworks which employ an update of the form\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - v_{t+1}.\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      The Nesterov version is analogously modified.\n",
      " |  \n",
      " |      Moreover, the initial value of the momentum buffer is set to the\n",
      " |      gradient value at the first step. This is in contrast to some other\n",
      " |      frameworks that initialize it to all zeros.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize: bool = False, foreach: Optional[bool] = None, differentiable: bool = False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Args:\n",
      " |          closure (Callable, optional): A closure that reevaluates the model\n",
      " |              and returns the loss.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self) -> Dict[str, Any]\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_param_group(self, param_group: Dict[str, Any]) -> None\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      " |      trainable and added to the :class:`Optimizer` as training progresses.\n",
      " |      \n",
      " |      Args:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along with group\n",
      " |              specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Dict[str, Any]) -> None\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): optimizer state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a load_state_dict post-hook which will be called after\n",
      " |      :meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(optimizer) -> None\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used.\n",
      " |      \n",
      " |      The hook will be called with argument ``self`` after calling\n",
      " |      ``load_state_dict`` on ``self``. The registered hook can be used to\n",
      " |      perform post-processing after ``load_state_dict`` has loaded the\n",
      " |      ``state_dict``.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If True, the provided post ``hook`` will be fired before\n",
      " |              all the already registered post-hooks on ``load_state_dict``. Otherwise,\n",
      " |              the provided ``hook`` will be fired after all the already registered\n",
      " |              post-hooks. (default: False)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemoveableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a load_state_dict pre-hook which will be called before\n",
      " |      :meth:`~torch.optim.Optimizer.load_state_dict` is called. It should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(optimizer, state_dict) -> state_dict or None\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used and the\n",
      " |      ``state_dict`` argument is a shallow copy of the ``state_dict`` the user\n",
      " |      passed in to ``load_state_dict``. The hook may modify the state_dict inplace\n",
      " |      or optionally return a new one. If a state_dict is returned, it will be used\n",
      " |      to be loaded into the optimizer.\n",
      " |      \n",
      " |      The hook will be called with argument ``self`` and ``state_dict`` before\n",
      " |      calling ``load_state_dict`` on ``self``. The registered hook can be used to\n",
      " |      perform pre-processing before the ``load_state_dict`` call is made.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If True, the provided pre ``hook`` will be fired before\n",
      " |              all the already registered pre-hooks on ``load_state_dict``. Otherwise,\n",
      " |              the provided ``hook`` will be fired after all the already registered\n",
      " |              pre-hooks. (default: False)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemoveableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_state_dict_post_hook(self, hook: Callable[[ForwardRef('Optimizer'), Dict[str, Any]], Optional[Dict[str, Any]]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a state dict post-hook which will be called after\n",
      " |      :meth:`~torch.optim.Optimizer.state_dict` is called. It should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(optimizer, state_dict) -> state_dict or None\n",
      " |      \n",
      " |      The hook will be called with arguments ``self`` and ``state_dict`` after generating\n",
      " |      a ``state_dict`` on ``self``. The hook may modify the state_dict inplace or optionally\n",
      " |      return a new one. The registered hook can be used to perform post-processing\n",
      " |      on the ``state_dict`` before it is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If True, the provided post ``hook`` will be fired before\n",
      " |              all the already registered post-hooks on ``state_dict``. Otherwise,\n",
      " |              the provided ``hook`` will be fired after all the already registered\n",
      " |              post-hooks. (default: False)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemoveableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook: Callable[[ForwardRef('Optimizer')], NoneType], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a state dict pre-hook which will be called before\n",
      " |      :meth:`~torch.optim.Optimizer.state_dict` is called. It should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(optimizer) -> None\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used.\n",
      " |      The hook will be called with argument ``self`` before calling ``state_dict`` on ``self``.\n",
      " |      The registered hook can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If True, the provided pre ``hook`` will be fired before\n",
      " |              all the already registered pre-hooks on ``state_dict``. Otherwise,\n",
      " |              the provided ``hook`` will be fired after all the already registered\n",
      " |              pre-hooks. (default: False)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemoveableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_step_post_hook(self, hook: Callable[[typing_extensions.Self, Tuple[Any, ...], Dict[str, Any]], NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register an optimizer step post hook which will be called after optimizer step.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(optimizer, args, kwargs) -> None\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_step_pre_hook(self, hook: Callable[[typing_extensions.Self, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register an optimizer step pre hook which will be called before\n",
      " |      optimizer step. It should have the following signature::\n",
      " |      \n",
      " |          hook(optimizer, args, kwargs) -> None or modified args and kwargs\n",
      " |      \n",
      " |      The ``optimizer`` argument is the optimizer instance being used. If\n",
      " |      args and kwargs are modified by the pre-hook, then the transformed\n",
      " |      values are returned as a tuple containing the new_args and new_kwargs.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  state_dict(self) -> Dict[str, Any]\n",
      " |      Returns the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * ``state``: a Dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes, but some common characteristics\n",
      " |          hold. For example, state is saved per parameter, and the parameter\n",
      " |          itself is NOT saved. ``state`` is a Dictionary mapping parameter ids\n",
      " |          to a Dict with state corresponding to each parameter.\n",
      " |      * ``param_groups``: a List containing all parameter groups where each\n",
      " |          parameter group is a Dict. Each parameter group contains metadata\n",
      " |          specific to the optimizer, such as learning rate and weight decay,\n",
      " |          as well as a List of parameter IDs of the parameters in the group.\n",
      " |      \n",
      " |      NOTE: The parameter IDs may look like indices but they are just IDs\n",
      " |      associating state with param_group. When loading from a state_dict,\n",
      " |      the optimizer will zip the param_group ``params`` (int IDs) and the\n",
      " |      optimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to\n",
      " |      match state WITHOUT additional verification.\n",
      " |      \n",
      " |      A returned state dict might look something like:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          {\n",
      " |              'state': {\n",
      " |                  0: {'momentum_buffer': tensor(...), ...},\n",
      " |                  1: {'momentum_buffer': tensor(...), ...},\n",
      " |                  2: {'momentum_buffer': tensor(...), ...},\n",
      " |                  3: {'momentum_buffer': tensor(...), ...}\n",
      " |              },\n",
      " |              'param_groups': [\n",
      " |                  {\n",
      " |                      'lr': 0.01,\n",
      " |                      'weight_decay': 0,\n",
      " |                      ...\n",
      " |                      'params': [0]\n",
      " |                  },\n",
      " |                  {\n",
      " |                      'lr': 0.001,\n",
      " |                      'weight_decay': 0.5,\n",
      " |                      ...\n",
      " |                      'params': [1, 2, 3]\n",
      " |                  }\n",
      " |              ]\n",
      " |          }\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Resets the gradients of all optimized :class:`torch.Tensor` s.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              This will in general have lower memory footprint, and can modestly improve performance.\n",
      " |              However, it changes certain behaviors. For example:\n",
      " |              1. When the user tries to access a gradient and perform manual ops on it,\n",
      " |              a None attribute or a Tensor full of 0s will behave differently.\n",
      " |              2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
      " |              are guaranteed to be None for params that did not receive a gradient.\n",
      " |              3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
      " |              (in one case it does the step with a gradient of 0 and in the other it skips\n",
      " |              the step altogether).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  profile_hook_step(func: Callable[[~_P], ~R]) -> Callable[[~_P], ~R]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  OptimizerPostHook = typing.Callable[[typing_extensions.Self, typing......\n",
      " |  \n",
      " |  OptimizerPreHook = typing.Callable[[typing_extensions.Self, typing.......\n",
      " |  \n",
      " |  __annotations__ = {'OptimizerPostHook': typing_extensions.TypeAlias, '...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1.8636374388147914e-10\n",
      "epoch: 50 loss: 1.8636374388147914e-10\n",
      "epoch: 100 loss: 1.8636374388147914e-10\n",
      "epoch: 150 loss: 1.8636374388147914e-10\n",
      "epoch: 200 loss: 1.8636374388147914e-10\n",
      "epoch: 250 loss: 1.8636374388147914e-10\n",
      "epoch: 300 loss: 1.8636374388147914e-10\n",
      "epoch: 350 loss: 1.8636374388147914e-10\n",
      "epoch: 400 loss: 1.8636374388147914e-10\n",
      "epoch: 450 loss: 1.8636374388147914e-10\n",
      "epoch: 500 loss: 1.8636374388147914e-10\n",
      "epoch: 550 loss: 1.8636374388147914e-10\n",
      "epoch: 600 loss: 1.8636374388147914e-10\n",
      "epoch: 650 loss: 1.8636374388147914e-10\n",
      "epoch: 700 loss: 1.8636374388147914e-10\n",
      "epoch: 750 loss: 1.8636374388147914e-10\n",
      "epoch: 800 loss: 1.8636374388147914e-10\n",
      "epoch: 850 loss: 1.8636374388147914e-10\n",
      "epoch: 900 loss: 1.8636374388147914e-10\n",
      "epoch: 950 loss: 1.8636374388147914e-10\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # 数据类型转换\n",
    "    inputs = torch.from_numpy(x).to(device)  # 由于x是ndarray数组，需要转换成tensor类型，如果用gpu训练，则会通过to函数把数据传入gpu\n",
    "    labels = torch.from_numpy(y).to(device)\n",
    "\n",
    "    # 训练\n",
    "    optimizer.zero_grad()  # 每次求偏导都会清零，否则会进行叠加\n",
    "    outputs = regression_model(inputs)  # 把输入传入定义的线性回归模型中，进行前向传播，得到预测结果\n",
    "    loss = criterion(outputs, labels)  # 通过均方误差评估预测误差\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新权重参数\n",
    "\n",
    "    # 每50次循环打印一次结果\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"epoch:\", epoch, \"loss:\", loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.9999743],\n",
       "       [ 6.999978 ],\n",
       "       [ 8.999983 ],\n",
       "       [10.999987 ],\n",
       "       [12.99999  ],\n",
       "       [14.999994 ],\n",
       "       [16.999998 ],\n",
       "       [19.000002 ],\n",
       "       [21.000006 ],\n",
       "       [23.00001  ],\n",
       "       [25.000013 ]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model(torch.from_numpy(x).to(device).requires_grad_()).data.cpu().numpy() # 通过训练好的模型预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'defaults': {'lr': 0.01,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'differentiable': False},\n",
       " '_optimizer_step_pre_hooks': OrderedDict(),\n",
       " '_optimizer_step_post_hooks': OrderedDict(),\n",
       " '_optimizer_state_dict_pre_hooks': OrderedDict(),\n",
       " '_optimizer_state_dict_post_hooks': OrderedDict(),\n",
       " '_optimizer_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_optimizer_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_zero_grad_profile_name': 'Optimizer.zero_grad#SGD.zero_grad',\n",
       " 'state': defaultdict(dict,\n",
       "             {Parameter containing:\n",
       "              tensor([[2.0000]], device='mps:0', requires_grad=True): {'momentum_buffer': None},\n",
       "              Parameter containing:\n",
       "              tensor([3.0000], device='mps:0', requires_grad=True): {'momentum_buffer': None}}),\n",
       " 'param_groups': [{'params': [Parameter containing:\n",
       "    tensor([[2.0000]], device='mps:0', requires_grad=True),\n",
       "    Parameter containing:\n",
       "    tensor([3.0000], device='mps:0', requires_grad=True)],\n",
       "   'lr': 0.01,\n",
       "   'momentum': 0,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'differentiable': False}],\n",
       " '_warned_capturable_if_run_uncaptured': True}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(regression_model.state_dict(), \"model.pk1\")  # 保存模型\n",
    "result = regression_model.load_state_dict(torch.load(\"model.pk1\"))  # 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = t.tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(b.tolist(), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_([3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: tensor([[0, 1, 2, 3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取对角线的元素\n",
    "index = t.LongTensor([[0, 1, 2, 3]])\n",
    "print(f'index: {index}')\n",
    "a.gather(0, index)  # dim=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]], device='mps:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 3., 3.]), tensor([[3., 3., 3.]]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(3, 3)\n",
    "b.sum(dim=0), b.sum(dim=0, keepdim=True)  # 前者输出形状是(3)，后者输出形状是(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  6,  8, 10],\n",
       "        [12, 15, 18, 21],\n",
       "        [24, 28, 32, 36]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(a, a.cumsum(dim=0))  # 对第二个维度行的元素按照索引顺序进行累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.ones(3, 2)\n",
    "b = t.ones(2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2.],\n",
       "         [2., 2.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[2., 2.],\n",
       "         [2., 2.],\n",
       "         [2., 2.]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 91.6 ms, sys: 1.07 ms, total: 92.7 ms\n",
      "Wall time: 91.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans = 0\n",
    "for i in range(1000000):\n",
    "    ans += i;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.backends.mps.is_available:\n",
    "    a = t.zeros(1)\n",
    "    t.save(a, \"a.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载为b，存储于gpu1上（因为保存时tensor就在gpu1上）\n",
    "b = t.load('a.pkl')\n",
    "# 加载为c，存储于cpu\n",
    "c = t.load('a.pkl', map_location=lambda storage, loc: storage)\n",
    "# 加载为d，存储于gpu0上\n",
    "d = t.load('a.pkl', map_location={'cuda:1': 'cuda:0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable as V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = V(t.ones(3, 4), requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = V(t.zeros(3, 4))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c = a.add(b)  # variable 函数的使用和 tensor 一致，等同于 c=a+b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, True)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注：虽然没有指定 c 需要求导，但 c 依赖于 a，由于 a 需要求导，因此 c 的 requeires_grad 默认设置为 True\n",
    "a.requires_grad, b.requires_grad, c.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注：`c.data.sum()`是在去 data 后变为 tensor，从 tensor 计算sum；`c.sum()`计算后仍然是 variable\n",
    "d = c.sum()\n",
    "d.backward()  # 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ll/g9yk3qbd3qnc06sfwm1s4t840000gn/T/ipykernel_81428/712901659.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  display(a.grad, c.grad, d.grad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(a.grad, c.grad, d.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0483e+01, 4.8500e-05, 2.2099e-01, 1.2519e-01],\n",
       "        [6.3724e-03, 9.8387e-05, 1.0509e-01, 5.3930e+00],\n",
       "        [5.4316e+00, 1.8271e-01, 2.6152e+00, 4.5434e-01]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    \"\"\"计算 y\"\"\"\n",
    "    y = x**2 * t.exp(x)\n",
    "    return y\n",
    "\n",
    "\n",
    "def grad_f(x):\n",
    "    \"\"\"手动对函数求导\"\"\"\n",
    "    dx = 2 * x * t.exp(x) + x**2 * t.exp(x)\n",
    "    return dx\n",
    "\n",
    "\n",
    "x = V(t.randn(3, 4), requires_grad=True)\n",
    "y = f(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4306e+01,  1.4025e-02, -4.5801e-01, -4.4238e-01],\n",
       "        [ 1.7228e-01,  2.0035e-02, -4.2708e-01,  1.4052e+01],\n",
       "        [ 1.4133e+01, -4.6090e-01,  7.9137e+00, -2.5815e-01]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_grad_variables = t.ones(\n",
    "    y.size())  # 由于dz/dy=1，并且grad_variables 形状需要与 y 一致，详解看下面的 3.4 小节\n",
    "y.backward(y_grad_variables)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4306e+01,  1.4025e-02, -4.5801e-01, -4.4238e-01],\n",
       "        [ 1.7228e-01,  2.0035e-02, -4.2708e-01,  1.4052e+01],\n",
       "        [ 1.4133e+01, -4.6090e-01,  7.9137e+00, -2.5815e-01]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f(x)  # autograd 计算的结果和利用公式计算的结果一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = V(t.ones(1))\n",
    "b = V(t.rand(1), requires_grad=True)\n",
    "w = V(t.rand(1), requires_grad=True)\n",
    "y = w * x  # 等价于 y=w.mul(x)\n",
    "z = y + b  # 等价于 z=y.add(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True, True, True)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于 y 依赖于求导的 w，故而即使 y 没有指定requires_grad=True，也为 True；z 同理\n",
    "x.requires_grad, b.requires_grad, w.requires_grad, y.requires_grad, z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, False, False)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_leaf, b.is_leaf, w.is_leaf, y.is_leaf, z.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f7dab3aa1f0>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<MulBackward0 at 0x7f7dab3cad30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f7dab3aa1f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grad_fn 可以查看这个 variable 的反向传播函数\n",
    "# 由于 z 是 add 函数的输出，所以它的反向传播函数是 AddBackward\n",
    "display(x.grad_fn, b.grad_fn ,w.grad_fn, y.grad_fn, z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x7f7dab3cad30>, 0),\n",
       " (<AccumulateGrad at 0x7f7dab3ca160>, 0))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next_functions 保存 grad_fn 的输入，grad_fn 是一个元组\n",
    "# 第一个是 y，它是乘法的输出，所以对应的反向传播函数 y.grad_fn 是 MulBackward\n",
    "# 第二个是 b，它是叶子节点，由用户创建，grad_fn 为 None，但是有 z.grad_fn.next_functions\n",
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable as V\n",
    "# 不是 jupyter 运行请注释掉下面一行，为了 jupyter 显示图片\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "t.manual_seed(1000)  # 随机数种子\n",
    "\n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    \"\"\"产生随机数据：y = x * 2 + 3，同时加上了一些噪声\"\"\"\n",
    "    x = t.rand(batch_size, 1) * 20\n",
    "    y = x * 2 + (1 + t.randn(batch_size, 1)) * 3  # 噪声为 |3-((1 + t.randn(batch_size, 1)) * 3)|\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# 查看 x，y 的分布情况\n",
    "x, y = get_fake_data()\n",
    "plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "w = V(t.rand(1, 1), requires_grad=True)\n",
    "b = V(t.zeros(1, 1), requires_grad=True)\n",
    "\n",
    "lr = 0.001  # 学习率\n",
    "\n",
    "for i in range(8000):\n",
    "    x, y = get_fake_data()\n",
    "    x, y = V(x), V(y)\n",
    "\n",
    "    # forwad：计算 loss\n",
    "    y_pred = x.mm(w) + b.expand_as(y)\n",
    "    loss = 0.5 * (y_pred - y)**2\n",
    "    loss = loss.sum()\n",
    "\n",
    "    # backward：自动计算梯度\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    w.data.sub_(lr * w.grad.data)\n",
    "    b.data.sub_(lr * b.grad.data)\n",
    "\n",
    "    # 梯度清零，不清零则会进行叠加，影响下一次梯度计算\n",
    "    w.grad.data.zero_()\n",
    "    b.grad.data.zero_()\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0, 20, dtype=t.float).view(-1, 1)\n",
    "        y = x.mm(w.data) + b.data.expand_as(x)\n",
    "        plt.plot(x.numpy(), y.numpy(), color='red')  # 预测效果\n",
    "\n",
    "        x2, y2 = get_fake_data(batch_size=20)\n",
    "        plt.scatter(x2.numpy(), y2.numpy(), color='blue')  # 真实数据\n",
    "\n",
    "        plt.xlim(0, 20)\n",
    "        plt.ylim(0, 41)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        break  # 注销这一行，可以看到动态效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
